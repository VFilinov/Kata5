# Logs------------------------------------------------------------------------------------

logSearchInfo = false
logMoves = false
logGamesEvery = 100
#logThreadsEvery = 10 # log on run threads 
logThreadsEvery = 24 # log on run threads 
#maxTryTimes = 20 #try find balanced move
logToStdout = true
logOpenings = false
logGenerate = false
logConfigContents = false

# Data writing-----------------------------------------------------------------------------------

dataBoardLen = 15

maxDataQueueSize = 2000
maxRowsPerTrainFile = 10000
firstFileRandMinProp = 0.15

# Fancy game selfplay settings--------------------------------------------------------------------

# These take dominance - if a game is forked for any of these reasons, that will be the next game played.
# For forked games, randomization of rules and "initGamesWithPolicy" is disabled, komi is made fair with prob "forkCompensateKomiProb".
#earlyForkGameProb = 0.04  # Fork to try alternative opening variety with this probability
#earlyForkGameExpectedMoveProp = 0.025  # Fork roughly within the first (boardArea * this) many moves
#forkGameProb = 0.01 # Fork to try alternative crazy move anywhere in game with this probability if not early forking
#forkGameMinChoices = 3   # Choose from the best of at least this many random choices
#earlyForkGameMaxChoices = 12  # Choose from the best of at most this many random choices
#forkGameMaxChoices = 36  # Choose from the best of at most this many random choices
# Otherwise, with this probability, learn a bit more about the differing evaluation of seki in different rulesets.
#sekiForkHackProb = 0.02

# Otherwise, play some proportion of games starting from SGF positions, with randomized rules (ignoring the sgf rules)
# On SGF positions, high temperature policy init is allowed
# startPosesProb = 0.0  # Play this proportion of games starting from SGF positions
# startPosesFromSgfDir = DIRECTORYPATH  # Load SGFs from this dir
# startPosesLoadProb = 1.0  # Only load each position from each SGF with this chance (save memory)
# startPosesTurnWeightLambda = 0  # 0 = equal weight  0.01 = decrease probability by 1% per turn  -0.01 = increase probability by 1% per turn.
# startPosesPolicyInitAreaProp = 0.0  # Same as policyInitAreaProp but for SGF positions

# Otherwise, play some proportion of games starting from hint positions (generated using "dataminesgfs" command), with randomized rules.
# On hint positions, "initGamesWithPolicy" does not apply.
# hintPosesProb = 0.0
# hintPosesDir = DIRECTORYPATH

# Otherwise we are playing a "normal" game, potentially with handicap stones, depending on "handicapProb", and
# potentially with komi randomization, depending on things like "komiStdev", and potentially with different
# board sizes, etc.

# Most of the remaining parameters here below apply regardless of the initialization, although a few of them
# vary depending on handicap vs normal game, and some are explicitly disabled (e.g. initGamesWithPolicy on hint positions).

# Generate-----------------------------------------------------------------------------------

# Generate with policy
initGamesWithPolicy = true  # Play the first few moves of a game high-temperaturely from policy
policyInitGaussMoveNum = 3
policyInitAvgMoveNum = 6 # The avg number of moves to play
# Generate with algoritm AvgDist
initGamesWithAvgDist = true
policyInitProb = 85.0 #initGamesWithPolicy/initGamesWithAvgDist*100.0 
# add balance move
addBalanceMoveProb = 50.0
noResultRandRadius = 0.95
#moveLimitProb = 0.05 

allowEarlyDraw = false
#earlyDrawThreshold = 0.99
#earlyDrawConsecTurns = 4
#earlyDrawProbSelfplay = 0.9

sidePositionProb = 0.02  # With this probability, train on refuting bad alternative moves.

cheapSearchProb = 0.75  # Do cheap searches with this probaiblity

cheapSearchVisits = 100  # Number of visits for cheap search
cheapSearchTargetWeight = 0.0  # Training weight for cheap search

reduceVisits = true
reduceVisitsThreshold = 0.9
reduceVisitsThresholdLookback = 3
reducedVisitsMin = 100
reducedVisitsWeight = 0.1

#handicapAsymmetricPlayoutProb = 0.5  # In handicap games, play with unbalanced players with this probablity
normalAsymmetricPlayoutProb = 0.01  # In regular games, play with unbalanced players with this probability
#normalAsymmetricPlayoutProb = 0
maxAsymmetricRatio = 8.0 # Max ratio to unbalance visits between players
#maxAsymmetricRatio = 4.0 # Max ratio to unbalance visits between players
#minAsymmetricCompensateKomiProb = 0.4 # Compensate komi with at least this probability for unbalanced players

policySurpriseDataWeight = 0.5  # This proportion of training weight should be concentrated on surprising moves
valueSurpriseDataWeight = 0.1   # This proportion of training weight should be concentrated on surprising position results

#estimateLeadProb = 0.05 # Train lead, rather than just scoremean. Consumes a decent number of extra visits, can be quite slow using low visits to set too high.
switchNetsMidGame = false  # When a new neural net is loaded, switch to it immediately instead of waiting for new game
#fancyKomiVarying = true  # In non-compensated handicap and fork games, vary komi to better learn komi and large score differences that would never happen in even games.

# Match-----------------------------------------------------------------------------------

#numGameThreads = 4
numGameThreads = 48
#numGameThreads = 80
maxMovesPerGame = 255

# Rules------------------------------------------------------------------------------------

# basicRules = RENJU,STANDARD,FREESTYLE
basicRules = RENJU,RENJU,RENJU,RENJU,STANDARD
VCNRules = NOVC
firstPassWinRules = false

useVCFInput = true
#VCFProb = 0.9
VCFProb = 0.99
useForbiddenInput = true
#ForbiddenProb = 0.5
ForbiddenProb = 0.99
#suppressPass=false

#bSizes = 15,19
bSizes = 15
# bSizeRelProbs = 90,10
bSizeRelProbs = 100
# allowRectangleProb = 0.00 # Play game on rectangular board with this probability


# Search limits-----------------------------------------------------------------------------------

maxVisits = 500
#numSearchThreads = 16
numSearchThreads = 1

# GPU Settings-------------------------------------------------------------------------------

nnMaxBatchSize = 128
nnCacheSizePowerOfTwo = 21
nnMutexPoolSizePowerOfTwo = 15
nnRandomize = true

# CUDA GPU settings--------------------------------------
# cudaDeviceToUse = 0 #use device 0 for all server threads (numNNServerThreadsPerModel) unless otherwise specified per-model or per-thread-per-model
# cudaDeviceToUseModel0 = 3 #use device 3 for model 0 for all threads unless otherwise specified per-thread for this model
# cudaDeviceToUseModel1 = 2 #use device 2 for model 1 for all threads unless otherwise specified per-thread for this model
# cudaDeviceToUseModel0Thread0 = 3 #use device 3 for model 0, server thread 0
# cudaDeviceToUseModel0Thread1 = 2 #use device 2 for model 0, server thread 1

trtUseFP16 = auto
trtUseNHWC = auto

# Root move selection and biases------------------------------------------------------------------------------

chosenMoveTemperatureEarly = 0.75
chosenMoveTemperatureHalflife = 19
chosenMoveTemperature = 0.15
chosenMoveSubtract = 0
chosenMovePrune = 1

rootNoiseEnabled = true
rootDirichletNoiseTotalConcentration = 10.83
rootDirichletNoiseWeight = 0.25

rootDesiredPerChildVisitsCoeff = 2
rootNumSymmetriesToSample = 4

#useLcbForSelection = true
lcbStdevs = 5.0
minVisitPropForLCB = 0.15

# Internal params------------------------------------------------------------------------------

#winLossUtilityFactor = 1.0
#staticScoreUtilityFactor = 0.00
#dynamicScoreUtilityFactor = 0.40
#dynamicScoreCenterZeroWeight = 0.25
#dynamicScoreCenterScale = 0.50
#noResultUtilityForWhite = 0.0
#drawEquivalentWinsForWhite = 0.5

#rootEndingBonusPoints = 0.5
rootPruneUselessMoves = true

rootPolicyTemperatureEarly = 1.25
rootPolicyTemperature = 1.1

cpuctExploration = 1.1
#cpuctExploration = 1.0
cpuctExplorationLog = 0.0
#cpuctExplorationLog = 0.45
#cpuctExplorationBase = 500

#fpuReductionMax = 0.2
#rootFpuReductionMax = 0.0
#valueWeightExponent = 0.5

numNNServerThreadsPerModel = 1
#gpuToUseThread0 = 0   
#gpuToUseThread1 = 1 

numVirtualLossesPerThread = 1
